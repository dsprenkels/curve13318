; Multiplication macros for field elements (integers modulo 2^255 - 19)
;
; Author: Daan Sprenkels <hello@dsprenkels.com>

%ifndef FE12_MUL_MAC_
%define FE12_MUL_MAC_

%include "fe12_squeeze.mac"

%macro fe12x4_mul_preload 2
    ; Preload values that will later be used by fe12x4_mul_body
    %push fe12x4_mul_preload_ctx

    %xdefine A %1
    %xdefine B %2
    vmovapd ymm6, yword [A]         ; load A[0]
    vmovapd ymm7, yword [A+32]      ; load A[1]
    vmovapd ymm0, yword [B]         ; load B[0]
    vmovapd ymm1, yword [B+32]      ; load B[1]
    vmovapd ymm2, yword [B+64]      ; load B[2]
    vmovapd ymm3, yword [B+96]      ; load B[3]
    vmovapd ymm4, yword [B+128]     ; load B[4]
    vmovapd ymm5, yword [B+160]     ; load B[5]

    %pop fe12x4_mul_preload_ctx
%endmacro

%macro fe12x4_mul_body 4
    ; Multiply two field elements using subtractive karatsuba method: body
    ;
    ; Arguments:
    ;   - %0:       address to the product of A and B
    ;   - %1:       two vectorized field element operand A
    ;   - %2:       two vectorized field element operand B
    ;   - %3:       address to 768 (24*32) aligned bytes of scratch space
    %push fe12x4_mul_body_ctx

    %xdefine C          %1
    %xdefine A          %2
    %xdefine B          %3
    %xdefine l          %4
    %xdefine A6_shr     %4 + 32*11
    %xdefine h          %4 + 32*12
    %xdefine A11_shr    %4 + 32*23

    ; compute L
    ; ymm9 will stores intermediate values, ymm10..ymm15 will stores the accumulators
    ; round 1/6
    vmulpd ymm10, ymm6, ymm0        ; l0 := A[0] * B[0]
    vmovapd yword [l], ymm10        ; store l[0]
    vmulpd ymm11, ymm6, ymm1        ; l1 := A[0] * B[1]
    vmulpd ymm12, ymm6, ymm2        ; l2 := A[0] * B[2]
    vmulpd ymm13, ymm6, ymm3        ; l3 := A[0] * B[3]
    vmulpd ymm14, ymm6, ymm4        ; l4 := A[0] * B[4]
    vmulpd ymm15, ymm6, ymm5        ; l5 := A[0] * B[5]
    ; round 2/6
    vmulpd ymm9, ymm7, ymm0
    vaddpd ymm11, ymm11, ymm9       ; l1 += A[1] * B[0]
    vmovapd yword [l+32], ymm11     ; store l1
    vmulpd ymm9, ymm7, ymm1
    vaddpd ymm12, ymm12, ymm9       ; l2 += A[1] * B[1]
    vmulpd ymm9, ymm7, ymm2
    vaddpd ymm13, ymm13, ymm9       ; l3 += A[1] * B[2]
    vmulpd ymm9, ymm7, ymm3
    vaddpd ymm14, ymm14, ymm9       ; l4 += A[1] * B[3]
    vmulpd ymm9, ymm7, ymm4
    vaddpd ymm15, ymm15, ymm9       ; l5 += A[1] * B[4]
    vmulpd ymm10, ymm7, ymm5        ; l6 := A[1] * B[5]
    ; round 3/6
    vmovapd ymm8, yword [A+64]      ; load A[2]
    vmulpd ymm9, ymm8, ymm0
    vaddpd ymm12, ymm12, ymm9       ; l2 += A[2] * B[0]
    vmovapd yword [l+64], ymm12     ; store l2
    vmulpd ymm9, ymm8, ymm1
    vaddpd ymm13, ymm13, ymm9       ; l3 += A[2] * B[1]
    vmulpd ymm9, ymm8, ymm2
    vaddpd ymm14, ymm14, ymm9       ; l4 += A[2] * B[2]
    vmulpd ymm9, ymm8, ymm3
    vaddpd ymm15, ymm15, ymm9       ; l5 += A[2] * B[3]
    vmulpd ymm9, ymm8, ymm4
    vaddpd ymm10, ymm10, ymm9       ; l6 += A[2] * B[4]
    vmulpd ymm11, ymm8, ymm5        ; l7 := A[2] * B[5]
    ; round 4/6
    vmovapd ymm6, yword [A+96]      ; load A[3]
    vmulpd ymm9, ymm6, ymm0
    vaddpd ymm13, ymm13, ymm9       ; l3 += A[3] * B[0]
    vmovapd yword [l+96], ymm13     ; store l3
    vmulpd ymm9, ymm6, ymm1
    vaddpd ymm14, ymm14, ymm9       ; l4 += A[3] * B[1]
    vmulpd ymm9, ymm6, ymm2
    vaddpd ymm15, ymm15, ymm9       ; l5 += A[3] * B[2]
    vmulpd ymm9, ymm6, ymm3
    vaddpd ymm10, ymm10, ymm9       ; l6 += A[3] * B[3]
    vmulpd ymm9, ymm6, ymm4
    vaddpd ymm11, ymm11, ymm9       ; l7 += A[3] * B[4]
    vmulpd ymm12, ymm6, ymm5        ; l8 := A[3] * B[5]
    ; round 5/6
    vmovapd ymm7, yword [A+128]     ; load A[4]
    vmulpd ymm9, ymm7, ymm0
    vaddpd ymm14, ymm14, ymm9       ; l4 += A[4] * B[0]
    vmovapd yword [l+128], ymm14    ; store l4
    vmulpd ymm9, ymm7, ymm1
    vaddpd ymm15, ymm15, ymm9       ; l5 += A[4] * B[1]
    vmulpd ymm9, ymm7, ymm2
    vaddpd ymm10, ymm10, ymm9       ; l6 += A[4] * B[2]
    vmulpd ymm9, ymm7, ymm3
    vaddpd ymm11, ymm11, ymm9       ; l7 += A[4] * B[3]
    vmulpd ymm9, ymm7, ymm4
    vaddpd ymm12, ymm12, ymm9       ; l8 += A[4] * B[4]
    vmulpd ymm13, ymm7, ymm5        ; l9 := A[4] * B[5]
    ; round 6/6
    vmovapd ymm8, yword [A+160]     ; load A[5]
    vmulpd ymm9, ymm8, ymm0
    vaddpd ymm15, ymm15, ymm9       ; l5 += A[5] * B[0]
    vmovapd yword [l+160], ymm15    ; store l5
    vmulpd ymm9, ymm8, ymm1
    vaddpd ymm10, ymm10, ymm9       ; l6 += A[5] * B[1]
    vmovapd yword [l+192], ymm10    ; store l6
    vmulpd ymm9, ymm8, ymm2
    vaddpd ymm11, ymm11, ymm9       ; l7 += A[5] * B[2]
    vmovapd yword [l+224], ymm11    ; store l7
    vmulpd ymm9, ymm8, ymm3
    vaddpd ymm12, ymm12, ymm9       ; l8 += A[5] * B[3]
    vmovapd yword [l+256], ymm12    ; store l8
    vmulpd ymm9, ymm8, ymm4
    vaddpd ymm13, ymm13, ymm9       ; l9 += A[5] * B[4]
    vmovapd yword [l+288], ymm13    ; store l9
    vmulpd ymm14, ymm8, ymm5        ; l10:= A[5] * B[5]
    vmovapd yword [l+320], ymm14    ; store l10

    ; compute H
    ; We will:
    ;   - precompute the A6_shr,A11_shr,Bx_shr values and,
    ;   - compute the A7_shr..A10_shr values on the fly, because they can be done in parallel
    ;     with vmulpd instruction.
    ;
    ; In this part of the code I will apply a really specific optimization: For {A,B}x7-{A,B}x10,
    ; we know for sure that they are:
    ;   - either 0, and so exponent part of the double is '0b00000000000'
    ;   - 0 modulo 2^149 and smaller than 2^237 => as such, the 7'th exponent bit is always set.
    ;     Thus we can use a mask operation for these values to divide them by 2*128.
    ; Because logical operations (on port 5) do not occur elsewhere in this function, these masking
    ; operations are practically free.
    ;
    vmovapd ymm8, yword [rel .const_1_1p_neg128]
    vmovapd ymm9, yword [rel .const_unset_bit59_mask]
    vmulpd ymm6, ymm8, yword [A+192]   ;  A6_shr = 0x1p-128 * A[ 6];
    vmulpd ymm0, ymm8, yword [B+192]   ;  B6_shr = 0x1p-128 * B[ 6];
    vandpd ymm1, ymm9, yword [B+224]   ;  B7_shr = unset_bit59(B[7]);
    vandpd ymm2, ymm9, yword [B+256]   ;  B8_shr = unset_bit59(B[8]);
    vandpd ymm3, ymm9, yword [B+288]   ;  B9_shr = unset_bit59(B[9]);
    vandpd ymm4, ymm9, yword [B+320]   ; B10_shr = unset_bit59(B[10]);
    vmulpd ymm5, ymm8, yword [B+352]   ; B11_shr = 0x1p-128 * B[11];
    vmulpd ymm7, ymm8, yword [A+352]   ; A11_shr = 0x1p-128 * A[11];

    ; Just as in the computation of L, we will store the accumulators (h) in ymm10..ymm15
    ; round 1/6
    vmulpd ymm10, ymm6, ymm0            ; h0 :=  A6_shr *  B6_shr
    vmovapd yword [h], ymm10            ; store h0
    vmulpd ymm11, ymm6, ymm1            ; h1 :=  A6_shr *  B7_shr
    vmulpd ymm12, ymm6, ymm2            ; h2 :=  A6_shr *  B8_shr
    vmulpd ymm13, ymm6, ymm3            ; h3 :=  A6_shr *  B9_shr
    vmulpd ymm14, ymm6, ymm4            ; h4 :=  A6_shr * B10_shr
    vmulpd ymm15, ymm6, ymm5            ; h5 :=  A6_shr * B11_shr
    vmovapd yword [A6_shr], ymm6        ; spill A6_shr
    ; round 2/6
    vandpd ymm6, ymm9, yword [A+224]    ; load A7_shr
    vmulpd ymm8, ymm6, ymm0
    vaddpd ymm11, ymm11, ymm8           ; h1 +=  A7_shr *  B6_shr
    vmovapd yword [h+32], ymm11         ; store h1
    vmulpd ymm8, ymm6, ymm1
    vaddpd ymm12, ymm12, ymm8           ; h2 +=  A7_shr *  B7_shr
    vmulpd ymm8, ymm6, ymm2
    vaddpd ymm13, ymm13, ymm8           ; h3 +=  A7_shr *  B8_shr
    vmulpd ymm8, ymm6, ymm3
    vaddpd ymm14, ymm14, ymm8           ; h4 +=  A7_shr *  B9_shr
    vmulpd ymm8, ymm6, ymm4
    vaddpd ymm15, ymm15, ymm8           ; h5 +=  A7_shr * B10_shr
    vmulpd ymm10, ymm6, ymm5            ; h6 :=  A7_shr * B11_shr
    ; round 3/6
    vandpd ymm6, ymm9, yword [A+256]    ; load A8_shr
    vmulpd ymm8, ymm6, ymm0
    vaddpd ymm12, ymm12, ymm8           ; h2 +=  A8_shr *  B6_shr
    vmovapd yword [h+64], ymm12         ; store h2
    vmulpd ymm8, ymm6, ymm1
    vaddpd ymm13, ymm13, ymm8           ; h3 +=  A8_shr *  B7_shr
    vmulpd ymm8, ymm6, ymm2
    vaddpd ymm14, ymm14, ymm8           ; h4 +=  A8_shr *  B8_shr
    vmulpd ymm8, ymm6, ymm3
    vaddpd ymm15, ymm15, ymm8           ; h5 +=  A8_shr *  B9_shr
    vmulpd ymm8, ymm6, ymm4
    vaddpd ymm10, ymm10, ymm8           ; h6 +=  A8_shr * B10_shr
    vmulpd ymm11, ymm6, ymm5            ; h7 :=  A8_shr * B11_shr
    ; round 4/6
    vandpd ymm6, ymm9, yword [A+288]    ; load A9_shr
    vmulpd ymm8, ymm6, ymm0
    vaddpd ymm13, ymm13, ymm8           ; h3 +=  A9_shr *  B6_shr
    vmovapd yword [h+96], ymm13         ; store h3
    vmulpd ymm8, ymm6, ymm1
    vaddpd ymm14, ymm14, ymm8           ; h4 +=  A9_shr *  B7_shr
    vmulpd ymm8, ymm6, ymm2
    vaddpd ymm15, ymm15, ymm8           ; h5 +=  A9_shr *  B8_shr
    vmulpd ymm8, ymm6, ymm3
    vaddpd ymm10, ymm10, ymm8           ; h6 +=  A9_shr *  B9_shr
    vmulpd ymm8, ymm6, ymm4
    vaddpd ymm11, ymm11, ymm8           ; h7 +=  A9_shr * B10_shr
    vmulpd ymm12, ymm6, ymm5            ; h8 :=  A9_shr * B11_shr
    ; round 5/6
    vandpd ymm6, ymm9, yword [A+320]    ; load A10_shr
    vmulpd ymm8, ymm6, ymm0
    vaddpd ymm14, ymm14, ymm8           ; h4 += A10_shr *  B6_shr
    vmovapd yword [h+128], ymm14        ; store h4
    vmulpd ymm8, ymm6, ymm1
    vaddpd ymm15, ymm15, ymm8           ; h5 += A10_shr *  B7_shr
    vmulpd ymm8, ymm6, ymm2
    vaddpd ymm10, ymm10, ymm8           ; h6 += A10_shr *  B8_shr
    vmulpd ymm8, ymm6, ymm3
    vaddpd ymm11, ymm11, ymm8           ; h7 += A10_shr *  B9_shr
    vmulpd ymm8, ymm6, ymm4
    vaddpd ymm12, ymm12, ymm8           ; h8 += A10_shr * B10_shr
    vmulpd ymm13, ymm6, ymm5            ; h9 := A10_shr * B11_shr
    ; round 6/6                         ; (A11_shr is already in ymm7)
    vmulpd ymm8, ymm7, ymm0
    vaddpd ymm15, ymm15, ymm8           ; h5 += A11_shr *  B6_shr
    vmovapd yword [h+160], ymm15        ; store h5
    vmulpd ymm8, ymm7, ymm1
    vaddpd ymm10, ymm10, ymm8           ; h6 += A11_shr *  B7_shr
    vmovapd yword [h+192], ymm10        ; store h6
    vmulpd ymm8, ymm7, ymm2
    vaddpd ymm11, ymm11, ymm8           ; h7 += A11_shr *  B8_shr
    vmovapd yword [h+224], ymm11        ; store h7
    vmulpd ymm8, ymm7, ymm3
    vaddpd ymm12, ymm12, ymm8           ; h8 += A11_shr *  B9_shr
    vmovapd yword [h+256], ymm12        ; store h8
    vmulpd ymm8, ymm7, ymm4
    vaddpd ymm13, ymm13, ymm8           ; h9 += A11_shr * B10_shr
    vmovapd yword [h+288], ymm13        ; store h9
    vmulpd ymm14, ymm7, ymm5            ; h10 := A11_shr * B11_shr
    vmovapd yword [A11_shr], ymm7       ; spill A11_shr
    vmovapd yword [h+320], ymm14        ; store h10
    ; At this point, we have A11_shr,B6_shr..B11_shr in registers, A6_shr is spilled to the stack

    ; Compute M_hat and accumulation into C[6]..C[11]
    ;
    ; After each round of M_hat, one of the M_hat values is done, then we have everything we need
    ; to compute the corresponding limb in our product. To reduce pressure on ports 2 and 3 (for
    ; loads), we will compute these values ahead of time (in-between the rounds).
    ;
    ; round 1/6
    vmovapd ymm6, yword [A]             ; load A[0]
    vsubpd ymm6, ymm6, yword [A6_shr]   ; mA0 := A[0] - A6_shr
    vsubpd ymm0, ymm0, yword [B]        ; mB0 :=  B6_shr - B[0]
    vmulpd ymm10, ymm6, ymm0            ; m0 := mA0 * mB0
    vsubpd ymm1, ymm1, yword [B+32]     ; mB1 :=  B7_shr - B[1]
    vmulpd ymm11, ymm6, ymm1            ; m1 := mA0 * mB1
    vsubpd ymm2, ymm2, yword [B+64]     ; mB2 :=  B8_shr - B[2]
    vmulpd ymm12, ymm6, ymm2            ; m2 := mA0 * mB2
    vsubpd ymm3, ymm3, yword [B+96]     ; mB3 :=  B9_shr - B[3]
    vmulpd ymm13, ymm6, ymm3            ; m3 := mA0 * mB3
    vsubpd ymm4, ymm4, yword [B+128]    ; mB4 := B10_shr - B[4]
    vmulpd ymm14, ymm6, ymm4            ; m4 := mA0 * mB4
    vsubpd ymm5, ymm5, yword [B+160]    ; mB5 := B11_shr - B[5]
    vmulpd ymm15, ymm6, ymm5            ; m5 := mA0 * mB5
    ; compute C[6] := l6 + 0x1p+128 * (m0 + l0 + h0) + 0x26p0*h6
    vaddpd ymm10, ymm10, yword [l]
    vaddpd ymm10, ymm10, yword [h]
    vmovapd ymm7, yword [rel .const_1_1p_128]
    vmulpd ymm10, ymm10, ymm7
    vmovapd ymm8, yword [rel .const_38]
    vmulpd ymm6, ymm8, yword [h+192]
    vaddpd ymm6, ymm6, yword [l+192]
    vaddpd ymm10, ymm10, ymm6
    vmovapd yword [C+192], ymm10
    ; round 2/6
    vandpd ymm10, ymm9, yword [A+224]   ; load A7_shr (ymm9 still contains .const_unset_bit59_mask)
    vmovapd ymm6, yword [A+32]          ; load A[1]
    vsubpd ymm6, ymm6, ymm10            ; mA1 := A[1] - A7_shr
    vmulpd ymm10, ymm6, ymm0
    vaddpd ymm11, ymm11, ymm10          ; m1 += mA1 * mB0
    vmulpd ymm10, ymm6, ymm1
    vaddpd ymm12, ymm12, ymm10          ; m2 += mA1 * mB1
    vmulpd ymm10, ymm6, ymm2
    vaddpd ymm13, ymm13, ymm10          ; m3 += mA1 * mB2
    vmulpd ymm10, ymm6, ymm3
    vaddpd ymm14, ymm14, ymm10          ; m4 += mA1 * mB3
    vmulpd ymm10, ymm6, ymm4
    vaddpd ymm15, ymm15, ymm10          ; m5 += mA1 * mB4
    vmulpd ymm10, ymm6, ymm5            ; m6 := mA1 * mB5
    ; compute C[7] := l7 + 0x1p+128 * (m1 + l1 + h1) + 0x26p0*h7
    vaddpd ymm11, ymm11, yword [l+32]
    vaddpd ymm11, ymm11, yword [h+32]
    vmulpd ymm11, ymm11, ymm7
    vmulpd ymm6, ymm8, yword [h+224]
    vaddpd ymm6, ymm6, yword [l+224]
    vaddpd ymm11, ymm11, ymm6
    vmovapd yword [C+224], ymm11
    ; round 3/6
    vandpd ymm11, ymm9, yword [A+256]   ; load A8_shr
    vmovapd ymm6, yword [A+64]          ; load A[2]
    vsubpd ymm6, ymm6, ymm11            ; mA2 := A[2] - A8_shr
    vmulpd ymm11, ymm6, ymm0
    vaddpd ymm12, ymm12, ymm11          ; m2 += mA2 * mB0
    vmulpd ymm11, ymm6, ymm1
    vaddpd ymm13, ymm13, ymm11          ; m3 += mA2 * mB1
    vmulpd ymm11, ymm6, ymm2
    vaddpd ymm14, ymm14, ymm11          ; m4 += mA2 * mB2
    vmulpd ymm11, ymm6, ymm3
    vaddpd ymm15, ymm15, ymm11          ; m5 += mA2 * mB3
    vmulpd ymm11, ymm6, ymm4
    vaddpd ymm10, ymm10, ymm11          ; m6 += mA2 * mB4
    vmulpd ymm11, ymm6, ymm5            ; m7 := mA2 * mB5
    ; compute C[8] := l8 + 0x1p+128 * (m2 + l2 + h2) + 0x26p0*h8
    vaddpd ymm12, ymm12, yword [l+64]
    vaddpd ymm12, ymm12, yword [h+64]
    vmulpd ymm12, ymm12, ymm7
    vmulpd ymm6, ymm8, yword [h+256]
    vaddpd ymm6, ymm6, yword [l+256]
    vaddpd ymm12, ymm12, ymm6
    vmovapd yword [C+256], ymm12
    ; round 4/6
    vandpd ymm12, ymm9, yword [A+288]   ; load A9_shr
    vmovapd ymm6, yword [A+96]          ; load A[3]
    vsubpd ymm6, ymm6, ymm12            ; mA3 := A[3] - A9_shr
    vmulpd ymm12, ymm6, ymm0
    vaddpd ymm13, ymm13, ymm12          ; m3 += mA3 * mB0
    vmulpd ymm12, ymm6, ymm1
    vaddpd ymm14, ymm14, ymm12          ; m4 += mA3 * mB1
    vmulpd ymm12, ymm6, ymm2
    vaddpd ymm15, ymm15, ymm12          ; m5 += mA3 * mB2
    vmulpd ymm12, ymm6, ymm3
    vaddpd ymm10, ymm10, ymm12          ; m6 += mA3 * mB3
    vmulpd ymm12, ymm6, ymm4
    vaddpd ymm11, ymm11, ymm12          ; m7 += mA3 * mB4
    vmulpd ymm12, ymm6, ymm5            ; m8 := mA3 * mB5
    ; compute C[9] = l9 + 0x1p+128 * (m3 + l3 + h3) + 0x26p0*h9
    vaddpd ymm13, ymm13, yword [l+96]
    vaddpd ymm13, ymm13, yword [h+96]
    vmulpd ymm13, ymm13, ymm7
    vmulpd ymm6, ymm8, yword [h+288]
    vaddpd ymm6, ymm6, yword [l+288]
    vaddpd ymm13, ymm13, ymm6
    vmovapd yword [C+288], ymm13
    ; round 5/6
    vandpd ymm13, ymm9, yword [A+320]   ; load A10_shr
    vmovapd ymm6, yword [A+128]         ; load A[4]
    vsubpd ymm6, ymm6, ymm13            ; mA4 := A[4] - A10_shr
    vmulpd ymm13, ymm6, ymm0
    vaddpd ymm14, ymm14, ymm13          ; m4 += mA4 * mB0
    vmulpd ymm13, ymm6, ymm1
    vaddpd ymm15, ymm15, ymm13          ; m5 += mA4 * mB1
    vmulpd ymm13, ymm6, ymm2
    vaddpd ymm10, ymm10, ymm13          ; m6 += mA4 * mB2
    vmulpd ymm13, ymm6, ymm3
    vaddpd ymm11, ymm11, ymm13          ; m7 += mA4 * mB3
    vmulpd ymm13, ymm6, ymm4
    vaddpd ymm12, ymm12, ymm13          ; m8 += mA4 * mB4
    vmulpd ymm13, ymm6, ymm5            ; m9 := mA4 * mB5
    ; compute C[10] := l10 + 0x1p+128 * (m4 + l4 + h4) + 0x26p0*h10
    vaddpd ymm14, ymm14, yword [l+128]
    vaddpd ymm14, ymm14, yword [h+128]
    vmulpd ymm14, ymm14, ymm7
    vmulpd ymm6, ymm8, yword [h+320]
    vaddpd ymm6, ymm6, yword [l+320]
    vaddpd ymm14, ymm14, ymm6
    vmovapd yword [C+320], ymm14
    ; round 6/6
    vmovapd ymm6, yword [A+160]         ; load A[5]
    vsubpd ymm6, ymm6, yword [A11_shr]  ; mA5 := A[5] - A11_shr
    vmulpd ymm14, ymm6, ymm0
    vaddpd ymm15, ymm15, ymm14          ; m5 += mA5 * mB0
    vmulpd ymm14, ymm6, ymm1
    vaddpd ymm10, ymm10, ymm14          ; m6 += mA5 * mB1
    vmulpd ymm14, ymm6, ymm2
    vaddpd ymm11, ymm11, ymm14          ; m7 += mA5 * mB2
    vmulpd ymm14, ymm6, ymm3
    vaddpd ymm12, ymm12, ymm14          ; m8 += mA5 * mB3
    vmulpd ymm14, ymm6, ymm4
    vaddpd ymm13, ymm13, ymm14          ; m9 += mA5 * mB4
    vmulpd ymm14, ymm6, ymm5            ; m10 := mA5 * mB5
    ; compute C[11] := 0x1p+128 * (m5 + l5 + h5)
    vaddpd ymm15, ymm15, yword [l+160]
    vaddpd ymm15, ymm15, yword [h+160]
    vmulpd ymm15, ymm15, ymm7
    vmovapd yword [C+352], ymm15

    ; compute C[{0..5}]
    ; We may be able to do the vandpd optimization here as well, but note
    ; that most pressure comes from vaddpd, not vmulpd. So we will not
    ; gain that much by optmizing out these vmulpd instructions.
    vmovapd ymm6, yword [rel .const_1_1p_neg128]
    vaddpd ymm10, ymm10, yword [l+192]
    vaddpd ymm10, ymm10, yword [h+192]
    vmulpd ymm10, ymm10, ymm6
    vaddpd ymm10, ymm10, yword [h]
    vmulpd ymm10, ymm10, ymm8
    vaddpd ymm0, ymm10, yword [l]
    vaddpd ymm11, ymm11, yword [l+224]
    vaddpd ymm11, ymm11, yword [h+224]
    vandpd ymm11, ymm11, ymm9
    vaddpd ymm11, ymm11, yword [h+32]
    vmulpd ymm11, ymm11, ymm8
    vaddpd ymm1, ymm11, yword [l+32]
    vaddpd ymm12, ymm12, yword [l+256]
    vaddpd ymm12, ymm12, yword [h+256]
    vandpd ymm12, ymm12, ymm9
    vaddpd ymm12, ymm12, yword [h+64]
    vmulpd ymm12, ymm12, ymm8
    vaddpd ymm2, ymm12, yword [l+64]
    vaddpd ymm13, ymm13, yword [l+288]
    vaddpd ymm13, ymm13, yword [h+288]
    vandpd ymm13, ymm13, ymm9
    vaddpd ymm13, ymm13, yword [h+96]
    vmulpd ymm13, ymm13, ymm8
    vaddpd ymm3, ymm13, yword [l+96]
    vaddpd ymm14, ymm14, yword [l+320]
    vaddpd ymm14, ymm14, yword [h+320]
    vmulpd ymm14, ymm14, ymm6
    vaddpd ymm14, ymm14, yword [h+128]
    vmulpd ymm14, ymm14, ymm8
    vaddpd ymm4, ymm14, yword [l+128]
    vmulpd ymm15, ymm8, yword [h+160]
    vaddpd ymm5, ymm15, yword [l+160]

    %pop fe12x4_mul_body_ctx
%endmacro

%macro fe12x4_mul_store_C_low 1
    ; store ymm{0-5} in C[{0-5}]
    ;
    ; Call this routine after fe12x4_mul_body, and you do not want to
    ; immediately squeeze the result.
    %push fe12x4_mul_store_C_low_ctx

    %xdefine C %1
    vmovapd yword [C], ymm0
    vmovapd yword [C+32], ymm1
    vmovapd yword [C+64], ymm2
    vmovapd yword [C+96], ymm3
    vmovapd yword [C+128], ymm4
    vmovapd yword [C+160], ymm5

    %pop fe12x4_mul_store_C_low_ctx
%endmacro

%macro fe12x4_mul_reload_C_high 1
    ; reload C[{6-11}] back into ymm{6-11}
    ;
    ; fe12_body needs to spill C[{6-11}] to the destination address, this
    ; macro reloads these values into the registers where the fe12_squeeze
    ; expects them to be.
    ;
    ; Call this routine if you have just called fe12x4_mul_body and intend
    ; to now squeeze the result using fe12x4_squeeze_noload.
    %push fe12x4_mul_reload_C_high_ctx
    %xdefine C %1

    vmovapd ymm6, yword [C+192]
    vmovapd ymm7, yword [C+224]
    vmovapd ymm8, yword [C+256]
    vmovapd ymm9, yword [C+288]
    vmovapd ymm10, yword [C+320]
    vmovapd ymm11, yword [C+352]

    %pop fe12x4_mul_reload_C_high_ctx
%endmacro

%macro fe12x4_mul 4
    ; Multiply two field elements method and squeeze the result
    ;
    ; Arguments:
    ;   - %0:       address to the product of A and B
    ;   - %1:       two vectorized field element operand A
    ;   - %2:       two vectorized field element operand B
    ;   - %3:       address to 768 (24*32) aligned bytes of scratch space
    %push fe12x4_mul_ctx

    %xdefine C          %1
    %xdefine A          %2
    %xdefine B          %3
    %xdefine scratch    %4

    fe12x4_mul_preload A, B
    fe12x4_mul_body C, A, B, scratch
    fe12x4_mul_reload_C_high C
    fe12x4_squeeze_noload C

    %pop fe12x4_mul_ctx
%endmacro

%macro fe12x4_mul_nopreload 4
    ; Multiply two field elements method and squeeze the result
    ;
    ; Arguments:
    ;   - %0:       address to the product of A and B
    ;   - %1:       two vectorized field element operand A
    ;   - %2:       two vectorized field element operand B
    ;   - %3:       address to 768 (24*32) aligned bytes of scratch space
    ;
    ; This macro skips the first loads from the inputs. Instead, the following loads are delegated
    ; to the caller. Note that the values must still be written to RAM. The caller must load the
    ; following values:
    ;   - B[0]: ymm0
    ;   - B[1]: ymm1
    ;   - B[2]: ymm2
    ;   - B[3]: ymm3
    ;   - B[4]: ymm4
    ;   - B[5]: ymm5
    ;   - A[0]: ymm6
    ;   - A[1]: ymm7


    %push fe12x4_mul_ctx

    %xdefine C          %1
    %xdefine A          %2
    %xdefine B          %3
    %xdefine scratch    %4

    fe12x4_mul_body C, A, B, scratch
    fe12x4_mul_reload_C_high C
    fe12x4_squeeze_noload C

    %pop fe12x4_mul_ctx
%endmacro

%macro fe12x4_mul_nosqueeze 4
    ; Multiply two field elements method and *do not* squeeze the result
    ;
    ; Arguments:
    ;   - %0:       address to the product of A and B
    ;   - %1:       two vectorized field element operand A
    ;   - %2:       two vectorized field element operand B
    ;   - %3:       address to 768 (24*32) aligned bytes of scratch space
    %push fe12x4_mul_nosqueeze_ctx

    %xdefine C          %1
    %xdefine A          %2
    %xdefine B          %3
    %xdefine scratch    %4

    fe12x4_mul_preload A, B
    fe12x4_mul_body C, A, B, scratch
    fe12x4_mul_store_C_low C

    %pop fe12x4_mul_nosqueeze_ctx
%endmacro

%macro fe12x4_mul_nosave 4
    ; Multiply two field elements method and *squeeze* but do not save the
    ; result to the stack.
    ;
    ; Use this routine if you intend to read the result from xmm{0..11}
    ; immediately after calling this macro.
    ;
    ; Arguments:
    ;   - %0:       address to the product of A and B
    ;   - %1:       two vectorized field element operand A
    ;   - %2:       two vectorized field element operand B
    ;   - %3:       address to 768 (24*32) aligned bytes of scratch space
    %push fe12x4_mul_nosave_ctx

    %xdefine C          %1
    %xdefine A          %2
    %xdefine B          %3
    %xdefine scratch    %4

    fe12x4_mul_preload A, B
    fe12x4_mul_body C, A, B, scratch
    fe12x4_mul_reload_C_high C
    fe12x4_squeeze_body

    %pop fe12x4_mul_nosave_ctx
%endmacro

%macro fe12x4_mul_consts 0
    ; The other macros in this file are dependent on these constants. If
    ; you call the other macros in this file, define these values after
    ; your call in the .rodata section.

    align 32,                        db 0
    .const_unset_bit59_mask: times 4 dq 0xF7FFFFFFFFFFFFFF
    .const_1_1p_neg128:      times 4 dq 0x1p-128
    .const_1_1p_128:         times 4 dq 0x1p+128
    .const_38:               times 4 dq 0x26p0
%endmacro

%endif
